{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_project.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSefDBrntVo"
      },
      "source": [
        "# !pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvkAU1HqRIuZ"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5hy6huKR6lG"
      },
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL9SbIC-RK6z"
      },
      "source": [
        "# !kaggle datasets download -d kazanova/sentiment140"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl9o-1CSRnkH"
      },
      "source": [
        "# !unzip '/content/sentiment140.zip' -d '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8CUgmgNfpsk"
      },
      "source": [
        "Required **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WZau5qbfcQ2"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from pandas import Series, DataFrame\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn import svm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from math import *\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import confusion_matrix\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dlxHRd8oj4y"
      },
      "source": [
        "data = pd.read_csv(\"/content/sample_data/training.1600000.processed.noemoticon.csv\", encoding = 'ISO-8859-1')\n",
        "data_2=data\n",
        "data_3=data\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFKpjArOoq4w"
      },
      "source": [
        "colmns = data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oClYh0kopHfx"
      },
      "source": [
        "data['text'] = data[data.columns[-1]]\n",
        "data['label'] = data[data.columns[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaDd2oxJpN2P"
      },
      "source": [
        "data = data.drop(columns=colmns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrLVSuT5pQHL"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1row8YUip1-g"
      },
      "source": [
        "def remove_spaces(text):\n",
        "  return re.sub(' +',' ',text)\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda text: remove_spaces(text))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAIoGNd9qCV7"
      },
      "source": [
        "# 2. Removing URLs\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda text: remove_urls(text))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISLfYvwoqRJU"
      },
      "source": [
        "# 3. Removing Emojies\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda text: remove_emoji(text))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8ivwEuBqSP2"
      },
      "source": [
        "\n",
        "\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda text: remove_punctuation(text))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN3jCZ8YqUxz"
      },
      "source": [
        "# 5. Removing multiple spaces\n",
        "\n",
        "\n",
        "data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join(text.split()))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFM6v5DJqWan"
      },
      "source": [
        "\n",
        "# 6. Removing non english words\n",
        "nltk.download('words')\n",
        "wds = set(nltk.corpus.words.words())\n",
        "\n",
        "data['text'] = data['text'].apply(lambda text: \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
        "         if w.lower() in wds))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GhLaOk2qYTD"
      },
      "source": [
        "# 7. Removing numbers\n",
        "def rem_num(text):\n",
        "  return ''.join(i for i in text if not i.isdigit())\n",
        "\n",
        "data['text'] = data[\"text\"].apply(lambda text: rem_num(text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrQ-snKlqaSn"
      },
      "source": [
        "#Fake targets\n",
        "\n",
        "c=data.groupby('label')\n",
        "k=c.get_group(0)\n",
        "k.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_p74Z_bqera"
      },
      "source": [
        "# Real targets\n",
        "\n",
        "p=c.get_group(4)\n",
        "p.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQhVEwcdqlzW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGJWKIq9qguW"
      },
      "source": [
        "all_words = ' '.join([text for text in k.text])\n",
        "wordcloud = WordCloud(width= 800, height= 800, max_font_size = 115, collocations = False).generate(all_words)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M5SXlAFqmfq"
      },
      "source": [
        "all_words = ' '.join([text for text in p.text])\n",
        "wordcloud = WordCloud(width= 800, height= 800, max_font_size = 150, collocations = False).generate(all_words)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNpOcyS_rXCx"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrMrSiHcrnPx"
      },
      "source": [
        "tweets =  shuffle(data).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEaIr0CGrpQT"
      },
      "source": [
        "tokenized_tweet=tweets['text'].apply(lambda x: x.split())\n",
        "tokenized_tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFzI_7Zkrtit"
      },
      "source": [
        "\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts = cv.fit_transform(tweets['text'].values.astype('U'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23kQdnU6rxul"
      },
      "source": [
        "X=text_counts\n",
        "y=tweets['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=19)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7MHG3pjsHlx"
      },
      "source": [
        "using naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOAYPVxcr1a5"
      },
      "source": [
        "\n",
        "cnb = ComplementNB()\n",
        "cnb.fit(X_train, y_train)\n",
        "cross_cnb = cross_val_score(cnb, X, y,n_jobs = -1)\n",
        "print(\"Cross Validation score = \",cross_cnb)                \n",
        "print (\"Train accuracy ={:.2f}%\".format(cnb.score(X_train,y_train)*100))\n",
        "print (\"Test accuracy ={:.2f}%\".format(cnb.score(X_test,y_test)*100))\n",
        "train_acc_cnb=cnb.score(X_train,y_train)\n",
        "test_acc_cnb=cnb.score(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMgC1amxsU1E"
      },
      "source": [
        "print(\"plotting the best parameters\")\n",
        "data_cnb = [train_acc_cnb,test_acc_cnb]\n",
        "labels = ['Train Accuracy','Test Accuracy']\n",
        "plt.xticks(range(len(data_cnb)), labels)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy plot with best parameters')\n",
        "plt.bar(range(len(data_cnb)), data_cnb,color=['blue','darkorange']) \n",
        "Train_acc = mpatches.Patch(color='blue', label='Train_acc')\n",
        "Test_acc = mpatches.Patch(color='darkorange', label='Test_acc')\n",
        "plt.legend(handles=[Train_acc, Test_acc],loc='best')\n",
        "plt.gcf().set_size_inches(8, 8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeSQfth8sbUS"
      },
      "source": [
        "y_pred_cnb =cnb.predict(X_test)\n",
        "print(\"Confusion matrix : \")\n",
        "print(confusion_matrix(y_test,y_pred_cnb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHqIilYysb5B"
      },
      "source": [
        "print(classification_report(y_test, y_pred_cnb))\n",
        "\n",
        "roc_score_cnb=roc_auc_score(y_test, y_pred_cnb)\n",
        "print(\"Area Under the Curve = \",roc_score_cnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3qG3mvZsfer"
      },
      "source": [
        "f1_scre=f1_score(y_test, y_pred_cnb, average=\"macro\")\n",
        "final_f1S=f1_scre*100\n",
        "print(\"F1 score =\",final_f1S)\n",
        "print()\n",
        "print(\"**************************************\")\n",
        "print()\n",
        "f1_cnb=f1_score(y_test, y_pred_cnb, average=\"macro\")\n",
        "precisio_scre=precision_score(y_test, y_pred_cnb, average=\"macro\")\n",
        "finalPreScore=100*precisio_scre\n",
        "print(\"Precision score :  \",finalPreScore)\n",
        "print()\n",
        "print(\"******************************************\")\n",
        "precision_cnb=precision_score(y_test, y_pred_cnb, average=\"macro\")\n",
        "print(\"precision_cnb : \")\n",
        "print(precision_cnb)\n",
        "print(\"******************************************\")\n",
        "print()\n",
        "recScre=recall_score(y_test, y_pred_cnb, average=\"macro\")\n",
        "final_recal=100*recScre\n",
        "print(\"Recall score :\",final_recal)  \n",
        "recall_cnb=recall_score(y_test, y_pred_cnb, average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d91kzbOgfii5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1noPn7p7YZpY"
      },
      "source": [
        "Now WE will use tensorflow_based_model NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22UrQOONxbH0"
      },
      "source": [
        "data_2 = pd.read_csv(\"/content/sample_data/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\")\n",
        "data_2.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5WNfQkKyDdZ"
      },
      "source": [
        "data_2=data_2[['text','label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcHxUJZdyGl7"
      },
      "source": [
        "data_2['label'][data_2['label']==4]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXDP2_RIyJDZ"
      },
      "source": [
        "data_pos = data_2[data_2['label'] == 1]\n",
        "data_neg = data_2[data_2['label'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BmuM54FyKB6"
      },
      "source": [
        "data_pos = data_pos.iloc[:int(24500)]\n",
        "data_neg = data_neg.iloc[:int(24500)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8YBAJp2yOfM"
      },
      "source": [
        "data_2 = pd.concat([data_pos, data_neg])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjVrQFWlyRdX"
      },
      "source": [
        "data_2['text']=data_2['text'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3xa5PsSzfqi"
      },
      "source": [
        "def cleaning_email(data_2):\n",
        "    return re.sub('@[^\\s]+', ' ', data_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAhqAIQKzUeA"
      },
      "source": [
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNr1x9JJzZtd"
      },
      "source": [
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAB8nAPjz9Tg"
      },
      "source": [
        "def cleaning_numbers(data_2):\n",
        "    return re.sub('[0-9]+', '', data_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7sO3iFozkdh"
      },
      "source": [
        "def cleaning_URLs(data_2):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSq8P0IBHzEX"
      },
      "source": [
        "st = nltk.PorterStemmer()\n",
        "def stemming_on_text(data_2):\n",
        "    text = [st.stem(word) for word in data_2]\n",
        "    return data_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtnvS4ob0mmE"
      },
      "source": [
        "def tensorflow_based_model(): \n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(2000,50,input_length=max_len)(inputs)\n",
        "    layer = LSTM(63)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOE-3IHLzWKG"
      },
      "source": [
        "data_2['text']= data_2['text'].apply(lambda x: cleaning_punctuations(x))\n",
        "data_2['text'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpqeWhOGzcVM"
      },
      "source": [
        "data_2['text'] = data_2['text'].apply(lambda x: cleaning_repeating_char(x))\n",
        "data_2['text'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjwr5833zijb"
      },
      "source": [
        "data_2['text']= data_2['text'].apply(lambda x: cleaning_email(x))\n",
        "data_2['text'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdrtt6OSzpjG"
      },
      "source": [
        "data_2['text'] = data_2['text'].apply(lambda x: cleaning_URLs(x))\n",
        "data_2['text'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhwwqJpgz-YF"
      },
      "source": [
        "data_2['text'] = data_2['text'].apply(lambda x: cleaning_numbers(x))\n",
        "data_2['text'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn2FMJHW0BKm"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "data_2['text'] = data_2['text'].apply(tokenizer.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOibX0V-0Ed-"
      },
      "source": [
        "data_2['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxpXCi_b0JFJ"
      },
      "source": [
        "data_2['text']= data_2['text'].apply(lambda x: stemming_on_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1vur_g40MT7"
      },
      "source": [
        "data['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-KKzst70U2x"
      },
      "source": [
        "data_2['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN9fPNGl0X6Z"
      },
      "source": [
        "X=data_2.text\n",
        "y=data_2.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQCPEOhF0boR"
      },
      "source": [
        "max_len = 470\n",
        "tok = Tokenizer(num_words=1999)\n",
        "print(tok)\n",
        "tok.fit_on_texts(X)\n",
        "sequences = tok.texts_to_sequences(X)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juhr6Qcr0eTW"
      },
      "source": [
        "sequences_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ9a2FES0ihn"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3lwa1eI0rDz"
      },
      "source": [
        "model = tensorflow_based_model()\n",
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])  \n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPon8Ygv0uel"
      },
      "source": [
        "history=model.fit(X_train,Y_train,batch_size=84,epochs=3, validation_split=0.1)\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijw060vf0yVv"
      },
      "source": [
        "accr1 = model.evaluate(X_test,Y_test)\n",
        "print(accr1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVh3BcJ504ir"
      },
      "source": [
        "print(\"-----------the accuracy of the model on test data is given below-----------\")\n",
        "print(\"Test set\")\n",
        "print()\n",
        "print(\"Accuracy: \",accr1[1])\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF7_fyt205kB"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p29n3Xqc08IX"
      },
      "source": [
        "print('\\n')\n",
        "print(\"confusion matrix\")\n",
        "print('\\n')\n",
        "CR=confusion_matrix(Y_test, y_pred)\n",
        "print(CR)\n",
        "print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A9n4vYM1ABo"
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\n",
        "area = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='red', lw=1, label='ROC curve (area = %0.3f)' % area)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.title('ROC')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQeFVoDt3-3N"
      },
      "source": [
        "data_3 = pd.read_csv(\"/content/sample_data/training.1600000.processed.noemoticon.csv\", encoding = 'ISO-8859-1')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6aRiOW74Gjb"
      },
      "source": [
        "data_3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux_gU5GU4Rnb"
      },
      "source": [
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n",
        "data_3.columns = DATASET_COLUMNS\n",
        "data_3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llVFf8Qh4XwF"
      },
      "source": [
        "data_3.drop(['ids','date','flag','user'],axis = 1,inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzypkoTC4gvL"
      },
      "source": [
        "positif_data = data_3[data_3.target==4].iloc[:24500,:]\n",
        "\n",
        "negative_data = data_3[data_3.target==0].iloc[:900,:]\n",
        "print()\n",
        "\n",
        "print(\"**********positif_data**********\")\n",
        "print(positif_data.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"*********negative_data***********\")\n",
        "print(negative_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEDLQnUV4lHA"
      },
      "source": [
        "print(\"positif_data and negative_data\")\n",
        "tweet_df = pd.concat([positif_data,negative_data],axis = 0)\n",
        "tweet_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uqp6_Gl4x1T"
      },
      "source": [
        "100*tweet_df.isna().sum()/len(tweet_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Djehdo5Iw-"
      },
      "source": [
        "def cleanup_tweets(tweet_df):\n",
        "    tweet_df['clean_tweet'] = tweet_df['TweetText'].str.replace(\"@\", \"\")\n",
        "    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].str.replace(r\"http\\S+\", \"\")      \n",
        "    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: x.split())\n",
        "    stemmer = PorterStemmer()\n",
        "    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: ' '.join([w for w in x]))\n",
        "    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYz9Q9CzZmyS"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of_E-2AZ5NiH"
      },
      "source": [
        "cleanup_tweets(tweet_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhvae7K35QO5"
      },
      "source": [
        "ls=0\n",
        "def createWrdCloudForSentiment(target):\n",
        "    temp_df = pd.DataFrame() \n",
        "    if target == -1:\n",
        "        temp_df = tweet_df\n",
        "        ls=8\n",
        "    else:\n",
        "        temp_df = tweet_df[tweet_df.target==target]\n",
        "    words = \" \".join(temp_df.clean_tweet)\n",
        "    wrdcld = WordCloud(stopwords=STOPWORDS,\n",
        "                      background_color='black',\n",
        "                      width=1500,\n",
        "                      height=1000).generate(words)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(wrdcld)\n",
        "    plt.axis('off')\n",
        "    plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZpJcWLU5VV_"
      },
      "source": [
        "createWrdCloudForSentiment(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skhpgZCB5ZLm"
      },
      "source": [
        "print(\"***************tweets for positive sentiment*******************\")\n",
        "createWrdCloudForSentiment(4) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f8BOShN5eRQ"
      },
      "source": [
        "print(\"*************tweet for depressive sentiments*************\")\n",
        "createWrdCloudForSentiment(0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wovtnxo25jLV"
      },
      "source": [
        "def createPieChartFor(t_df):\n",
        "    a=len(t_df)\n",
        "    b=t_df.value_counts()/a\n",
        "    Lst = 100*b\n",
        "    \n",
        "    labels = t_df.value_counts().index.values\n",
        "    sizes =  Lst \n",
        "    \n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.pie(sizes, labels=labels, autopct='%1.2f%%', startangle=100)\n",
        "    ax1.axis('equal')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC3rWEmu5kaQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il4QfKxx5vPc"
      },
      "source": [
        "train_df, test_df = train_test_split(tweet_df, test_size=0.3, random_state=33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpsuoUFc5xTd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kETLQz-50fq"
      },
      "source": [
        "test_tweets =[]\n",
        "train_tweets =[]\n",
        "i=0\n",
        "for tweet in train_df.clean_tweet:\n",
        "    train_tweets.append(tweet)\n",
        "    i=+1\n",
        "    \n",
        "for tweet in test_df.clean_tweet:\n",
        "    test_tweets.append(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzx7CM4253CE"
      },
      "source": [
        "train_tweets[:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaMUTkTx55tV"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "train_tfidf_model = vectorizer.fit_transform(train_tweets)\n",
        "test_tfidf_model = vectorizer.transform(test_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50yocb-s5602"
      },
      "source": [
        "train_tfidf = pd.DataFrame(train_tfidf_model.toarray(), columns=vectorizer.get_feature_names())\n",
        "train_tfidf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2K5Xz9N6Atj"
      },
      "source": [
        "i=0\n",
        "j=0\n",
        "cls = [MultinomialNB(), \n",
        "       RandomForestClassifier(n_estimators=999, random_state=25),\n",
        "       svm.SVC()]\n",
        "accuracy = []\n",
        "cls_name = []\n",
        "lbl_actual = test_df.target\n",
        "for cl in cls:\n",
        "    j+=1\n",
        "    model = cl.fit(train_tfidf_model,train_df.target)\n",
        "    lbl_pred = model.predict(test_tfidf_model)\n",
        "    a = (100*accuracy_score(lbl_pred, lbl_actual))\n",
        "    a = round(a,2)\n",
        "    accuracy.append(a)\n",
        "    cls_name.append(cl.__class__.__name__)\n",
        "    print(\"********************\")\n",
        "    print(cls_name[i])\n",
        "    print(\"Accuracy Score : \")\n",
        "    print(a)\n",
        "    print(\"********************\")\n",
        "    print( classification_report(lbl_pred, lbl_actual))\n",
        "    i =i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMGnM4HH7goa"
      },
      "source": [
        "print(\"Accuracy for Naive Bayes, Random Forest Classifier and SVM\")\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDNTAUjm6E2V"
      },
      "source": [
        "plt.figure(figsize=(10,9))\n",
        "plt.bar(cls_name, accuracy,color='blue')\n",
        "plt.xticks(rotation=70)\n",
        "for index,data in enumerate(accuracy):\n",
        "    plt.text(x=index , y =data+1 , s=f\"{data}%\" , fontdict=dict(fontsize=11),color='red')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttG5y3qWRZkh"
      },
      "source": [
        "i = 0\n",
        "cls = [LogisticRegression(),\n",
        "       DecisionTreeClassifier(),\n",
        "       KNeighborsClassifier(n_neighbors = 5),\n",
        "       XGBClassifier(max_depth=5, n_estimators=999, nthread= 3)]\n",
        "accuracy = []\n",
        "cls_name = []\n",
        "lbl_actual = test_df.target\n",
        "\n",
        "\n",
        "for cl in cls:\n",
        "    model = cl.fit(train_tfidf_model,train_df.target)\n",
        "    lbl_pred = model.predict(test_tfidf_model)\n",
        "    a = (100*accuracy_score(lbl_pred, lbl_actual))\n",
        "    a = round(a,2)\n",
        "    accuracy.append(a)\n",
        "    cls_name.append(cl.__class__.__name__)\n",
        "    print(\"********************\")\n",
        "    print(cls_name[i])\n",
        "    print(\"Accuracy Score : \")\n",
        "    print(a)\n",
        "    print(\"********************\")\n",
        "    print( classification_report(lbl_pred, lbl_actual))\n",
        "    i =i+1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4olnP5eufQS"
      },
      "source": [
        "print(\"Accuracy for Logistic Regression, Decision Tree, K-Neighbors ,XG Boast\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXuyobFNRvgs"
      },
      "source": [
        "plt.figure(figsize=(10,9))\n",
        "plt.bar(cls_name, accuracy,color='blue')\n",
        "plt.xticks(rotation=70)\n",
        "for index,data in enumerate(accuracy):\n",
        "    plt.text(x=index , y =data+1 , s=f\"{data}%\" , fontdict=dict(fontsize=11),color='red')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}